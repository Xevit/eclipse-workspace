<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Using Git for configuration history</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/MqpxTxyYw0A/" /><category term="cli" scheme="searchisko:content:tags" /><category term="configuration" scheme="searchisko:content:tags" /><category term="feed_group_name_jbossas" scheme="searchisko:content:tags" /><category term="feed_name_wildfly" scheme="searchisko:content:tags" /><category term="history" scheme="searchisko:content:tags" /><category term="wildfly" scheme="searchisko:content:tags" /><author><name>Emmanuel Hugonnet</name></author><id>searchisko:content:id:jbossorg_blog-using_git_for_configuration_history</id><updated>2018-09-28T18:00:00Z</updated><published>2018-09-28T18:00:00Z</published><content type="html">&lt;div id="preamble"&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Until now the history of configuration in WildFly was using the folder + filename pattern. Now we have moved to a proper SCM integrating Git to manage history.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;You can now take advantage of a full Git support for your configuration history:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;every change in your configuration is now a commit.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;you can use branches to develop in parallel.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;you can create tags for stable points in your configuration.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;pull configuration from a remote repository.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;push your configuration history to a remote repository.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;use the git-bisect tool at your disposal when things go wrong.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Now if we execute a management operation that modifies the model, for example adding a new system property using the CLI:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code data-lang="ruby"&gt;[standalone&lt;span class="instance-variable"&gt;@localhost&lt;/span&gt;:&lt;span class="integer"&gt;9990&lt;/span&gt; /] /system-property=&lt;span class="key"&gt;test&lt;/span&gt;:add(value=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;test123&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;) {&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;outcome&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; =&amp;gt; &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;success&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;}&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;What happens is:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;The change is applied to the configuration file.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The configuration file is added to a new commit.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="admonitionblock important"&gt; &lt;table&gt; &lt;tr&gt; &lt;td class="icon"&gt; &lt;i class="fa icon-important" title="Important"&gt;&lt;/i&gt; &lt;/td&gt; &lt;td class="content"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The notion of configuration has been updated with the Git support. It covers more than &amp;apos;just&amp;apos; the &lt;code&gt;standalone.xml&lt;/code&gt; history but also the content files (aka managed deployments).&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Thus even your deployments are in history, which makes sense in a way since those deployments appear in the configuration file.&lt;/p&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_starting_with_a_local_git_repository"&gt;Starting with a local Git repository&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;To start using Git you don’t have to create the repository, WildFly can do that for you. Just start your server with the following command line:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code data-lang="bash"&gt;$ __WILDFLY_HOME__/bin/standalone.sh --git-repo=local --git-branch=my_branch&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;If a &lt;em&gt;--git-branch&lt;/em&gt; parameter is added then the repository will be checked out from the supplied branch. Please note that the branch will not be automatically created and must already exist in the repository. By default, if no parameter is specified, the branch &lt;code&gt;master&lt;/code&gt; will be used. If a &lt;em&gt;--git-branch&lt;/em&gt; parameter is added then the repository will be checked out from the supplied branch. Please note that the branch will not be automatically created and must already exist in the repository. By default, if no parameter is specified, the branch &lt;code&gt;master&lt;/code&gt; will be used.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_starting_with_a_remote_git_repository"&gt;Starting with a remote Git Repository&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;To start WildFly with a configuration from a remote Git repository is simple too, just use the following command line:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code data-lang="bash"&gt;$ __WILDFLY_HOME__/bin/standalone.sh --git-repo=https://github.com/USER_NAME/wildfly-config.git --git-branch=master&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="admonitionblock important"&gt; &lt;table&gt; &lt;tr&gt; &lt;td class="icon"&gt; &lt;i class="fa icon-important" title="Important"&gt;&lt;/i&gt; &lt;/td&gt; &lt;td class="content"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Be careful with this as the first step is to delete the configuration files to avoid conflicts when pulling for the first time.&lt;/p&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;div class="admonitionblock note"&gt; &lt;table&gt; &lt;tr&gt; &lt;td class="icon"&gt; &lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt; &lt;/td&gt; &lt;td class="content"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Note that you can use remote aliases if you have added them to your &lt;code&gt;.gitconfig&lt;/code&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_snapshots"&gt;Snapshots&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;In addition to the commits taken by the server as described above, you can manually take snapshots which will be stored as &lt;code&gt;tags&lt;/code&gt; in the Git repository.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The ability to take a snapshot has been enhanced to allow you to add a comment to it. This comment will be used when creating the Git tag.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This is how you can take a snapshot from the JBoss CLI tool:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code data-lang="ruby"&gt;[standalone&lt;span class="instance-variable"&gt;@localhost&lt;/span&gt;:&lt;span class="integer"&gt;9990&lt;/span&gt; /] &lt;span class="symbol"&gt;:take&lt;/span&gt;-snapshot(name=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;snapshot&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, comment=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;1st snapshot&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;) { &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;outcome&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; =&amp;gt; &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;success&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;result&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; =&amp;gt; &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;1st snapshot&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; }&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;You can also use the CLI to list all the snapshots:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code data-lang="ruby"&gt;[standalone&lt;span class="instance-variable"&gt;@localhost&lt;/span&gt;:&lt;span class="integer"&gt;9990&lt;/span&gt; /] &lt;span class="symbol"&gt;:list&lt;/span&gt;-snapshots { &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;outcome&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; =&amp;gt; &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;success&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;result&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; =&amp;gt; { &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;directory&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; =&amp;gt; &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;names&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; =&amp;gt; [ &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;snapshot : 1st snapshot&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;refs/tags/snapshot&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;snapshot2 : 2nd snapshot&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;refs/tags/snapshot2&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; ] } }&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;To delete a particular snapshot:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code data-lang="ruby"&gt;[standalone&lt;span class="instance-variable"&gt;@localhost&lt;/span&gt;:&lt;span class="integer"&gt;9990&lt;/span&gt; /] &lt;span class="symbol"&gt;:delete&lt;/span&gt;-snapshot(name=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;snapshot2&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;) {&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;outcome&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; =&amp;gt; &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;success&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;}&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="admonitionblock note"&gt; &lt;table&gt; &lt;tr&gt; &lt;td class="icon"&gt; &lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt; &lt;/td&gt; &lt;td class="content"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Note that this is a real Git repository, thus using the git client of your choice you can list those tags, or browse the history.&lt;/p&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_publishing"&gt;Publishing&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;You may &amp;apos;publish&amp;apos; your changes on a remote repository (provided you have write access to it) so you can share them. For example, if you want to publish on GitHub, you need to create a token and allow for &lt;em&gt;full control&lt;/em&gt; of the repository. Then use that token in an Elytron configuration file like this:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code data-lang="xml"&gt;&lt;span class="preprocessor"&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;configuration&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;authentication-client&lt;/span&gt; &lt;span class="attribute-name"&gt;xmlns&lt;/span&gt;=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;urn:elytron:1.1&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;span class="tag"&gt;&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;authentication-rules&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;rule&lt;/span&gt; &lt;span class="attribute-name"&gt;use-configuration&lt;/span&gt;=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;test-login&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;span class="tag"&gt;&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;/rule&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;/authentication-rules&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;authentication-configurations&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;configuration&lt;/span&gt; &lt;span class="attribute-name"&gt;name&lt;/span&gt;=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;test-login&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;span class="tag"&gt;&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;sasl-mechanism-selector&lt;/span&gt; &lt;span class="attribute-name"&gt;selector&lt;/span&gt;=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;BASIC&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; &lt;span class="tag"&gt;/&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;set-user-name&lt;/span&gt; &lt;span class="attribute-name"&gt;name&lt;/span&gt;=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;$GITHUB_USERNAME&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; &lt;span class="tag"&gt;/&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;credentials&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;clear-password&lt;/span&gt; &lt;span class="attribute-name"&gt;password&lt;/span&gt;=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;$GITHUB_TOKEN&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; &lt;span class="tag"&gt;/&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;/credentials&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;set-mechanism-realm&lt;/span&gt; &lt;span class="attribute-name"&gt;name&lt;/span&gt;=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;testRealm&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; &lt;span class="tag"&gt;/&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;/configuration&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;/authentication-configurations&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;/authentication-client&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;/configuration&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Then, to publish your changes:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code data-lang="ruby"&gt;[standalone&lt;span class="instance-variable"&gt;@localhost&lt;/span&gt;:&lt;span class="integer"&gt;9990&lt;/span&gt; /] &lt;span class="symbol"&gt;:publish&lt;/span&gt;-configuration(location=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;origin&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;) {&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;outcome&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; =&amp;gt; &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;success&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;}&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_references"&gt;References&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;For the official documentation regarding Git history : &lt;a href="http://docs.wildfly.org/14/Admin_Guide.html#Configuration_file_git_history"&gt;Official Documentation&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/MqpxTxyYw0A" height="1" width="1" alt=""/&gt;</content><summary>Until now the history of configuration in WildFly was using the folder + filename pattern. Now we have moved to a proper SCM integrating Git to manage history. You can now take advantage of a full Git support for your configuration history: every change in your configuration is now a commit. you can use branches to develop in parallel. you can create tags for stable points in your configuration. p...</summary><dc:creator>Emmanuel Hugonnet</dc:creator><dc:date>2018-09-28T18:00:00Z</dc:date><feedburner:origLink>http://wildfly.org/news/2018/09/28/Git-History/</feedburner:origLink></entry><entry><title>Dynamic IP address management in Open Virtual Network (OVN): Part Two</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/acOTlmbiDGU/" /><category term="cloud networking" scheme="searchisko:content:tags" /><category term="community" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="IP address management" scheme="searchisko:content:tags" /><category term="IPAM" scheme="searchisko:content:tags" /><category term="network function virtualization" scheme="searchisko:content:tags" /><category term="Networking" scheme="searchisko:content:tags" /><category term="NFV" scheme="searchisko:content:tags" /><category term="open virtual network" scheme="searchisko:content:tags" /><category term="Open vSwitch" scheme="searchisko:content:tags" /><category term="OVN" scheme="searchisko:content:tags" /><category term="Red Hat Enterprise Linux" scheme="searchisko:content:tags" /><category term="switches" scheme="searchisko:content:tags" /><category term="virtual networking" scheme="searchisko:content:tags" /><author><name>Mark Michelson</name></author><id>searchisko:content:id:jbossorg_blog-dynamic_ip_address_management_in_open_virtual_network_ovn_part_two</id><updated>2018-09-27T19:37:44Z</updated><published>2018-09-27T19:37:44Z</published><content type="html">&lt;p&gt;In &lt;a href="https://developers.redhat.com/blog/2018/09/03/ovn-dynamic-ip-address-management/"&gt;part one&lt;/a&gt; of this series, we explored the dynamic IP address management (IPAM) capabilities of Open Virtual Network. We covered the &lt;code&gt;subnet&lt;/code&gt;, &lt;code&gt;ipv6_prefix&lt;/code&gt;, and &lt;code&gt;exclude_ips&lt;/code&gt; options on logical switches. We then saw how these options get applied to logical switch ports whose addresses have been set to the special &amp;#8220;dynamic&amp;#8221; value.  OVN, a subproject of &lt;a href="https://developers.redhat.com/blog/tag/open-vswitch/"&gt;Open vSwitch&lt;/a&gt;, is used for virtual networking in a number of Red Hat products like Red Hat OpenStack Platform, Red Hat Virtualization, and &lt;a href="https://developers.redhat.com/products/openshift/overview/"&gt;Red Hat OpenShift Container Platform&lt;/a&gt; in a future release.&lt;/p&gt; &lt;p&gt;In this part, we&amp;#8217;re going to explore some of the oversights and downsides in the feature, how those have been corrected, and what&amp;#8217;s in store for OVN in future versions.&lt;/p&gt; &lt;p&gt;&lt;span id="more-517817"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Subnet changes&lt;/h2&gt; &lt;p&gt;Let&amp;#8217;s start by creating a simple logical switch with a couple of logical switch ports that use dynamic addresses:&lt;/p&gt; &lt;pre&gt;ovn-nbctl ls-add sw ovn-nbctl set Logical_Switch sw other_config:subnet=192.168.1.0/24 ovn-nbctl lsp-add sw sw-p1 ovn-nbctl lsp-set-addresses sw-p1 "dynamic" ovn-nbctl lsp-add sw sw-p2 ovn-nbctl lsp-set-addresses sw-p2 "dynamic" &lt;/pre&gt; &lt;p&gt;This creates a logical switch &lt;code&gt;sw&lt;/code&gt; with ports &lt;code&gt;sw-p1&lt;/code&gt; and &lt;code&gt;sw-p2&lt;/code&gt;. Port &lt;code&gt;sw-p1&lt;/code&gt; is assigned address 192.168.1.2, and port &lt;code&gt;sw-p2&lt;/code&gt; is assigned 192.168.1.3.&lt;/p&gt; &lt;p&gt;But wait: we made a mistake! We actually meant to set the subnet of the switch to &lt;code&gt;192.168.0.0/24&lt;/code&gt;. Let&amp;#8217;s correct it.&lt;/p&gt; &lt;pre&gt;ovn-nbctl set Logical_Switch sw other_config:subnet=192.168.0.0/24 &lt;/pre&gt; &lt;p&gt;All right, let&amp;#8217;s see how that&amp;#8217;s affected the logical switch ports&amp;#8217; addresses. If you are running the &lt;a href="http://www.openvswitch.org/"&gt;Open vSwitch&lt;/a&gt; (OVS) 2.9 series or earlier, then you&amp;#8217;ll see the following:&lt;/p&gt; &lt;pre&gt;$ ovn-nbctl --columns=name,dynamic_addresses,addresses list logical_switch_port name : "sw-p2" dynamic_addresses : "0a:00:00:00:00:02 192.168.1.3" addresses : [dynamic] name : "sw-p1" dynamic_addresses : "0a:00:00:00:00:01 192.168.1.2" addresses : [dynamic] &lt;/pre&gt; &lt;p&gt;Huh? The dynamic addresses didn&amp;#8217;t update. Prior to OVS version 2.10, the dynamic addresses will not automatically update if the &lt;code&gt;subnet&lt;/code&gt;, &lt;code&gt;ipv6_prefix&lt;/code&gt;, or &lt;code&gt;exclude_ips&lt;/code&gt; is updated. If you want the dynamic addresses to update, you need to clear the &lt;code&gt;dynamic_addresses&lt;/code&gt; from the affected logical switch ports. The easiest way to clear the &lt;code&gt;dynamic_addresses&lt;/code&gt; on all switch ports on switch &lt;code&gt;sw&lt;/code&gt; is the following:&lt;/p&gt; &lt;pre&gt;for port in $(ovn-nbctl --bare --columns=port find logical_switch name=sw) ; do ovn-nbctl clear logical_switch_port $port dynamic_addresses ; done &lt;/pre&gt; &lt;p&gt;Now let&amp;#8217;s take another look at the logical switch ports:&lt;/p&gt; &lt;pre&gt;$ ovn-nbctl --columns=name,dynamic_addresses,addresses list logical_switch_port name : "sw-p2" dynamic_addresses : "0a:00:00:00:00:03 192.168.0.2" addresses : [dynamic] name : "sw-p1" dynamic_addresses : "0a:00:00:00:00:04 192.168.0.3" addresses : [dynamic] &lt;/pre&gt; &lt;p&gt;There; that&amp;#8217;s better. There are a couple of things to note here. First, the order in which IP addresses get assigned to the switch ports is not always predictable. The final octet of the IP addresses assigned to the switch ports was swapped from what it had previously been. Also, the MAC addresses have been updated on each switch port. When we cleared the &lt;code&gt;dynamic_addresses&lt;/code&gt;, the MAC address assignments on the switch port got lost. As a result, &lt;code&gt;ovn-northd&lt;/code&gt; assigns new MAC addresses to the ports. Unfortunately, if you are using dynamic MAC addresses, this is unavoidable.&lt;/p&gt; &lt;p&gt;The good news is that starting with OVS 2.10.0, this is no longer necessary. Updating &lt;code&gt;subnet&lt;/code&gt;, &lt;code&gt;ipv6_prefix&lt;/code&gt;, or &lt;code&gt;exclude_ips&lt;/code&gt; on a logical switch will automatically update the &lt;code&gt;dynamic_addresses&lt;/code&gt; on all logical switch ports. The even better news is that only the affected values are updated, so in this particular case, the MAC addresses on each switch port stay the same.&lt;/p&gt; &lt;h2&gt;Conflicting addresses&lt;/h2&gt; &lt;p&gt;Let&amp;#8217;s take our switch from the previous section and add a third switch port to it:&lt;/p&gt; &lt;pre&gt;ovn-nbctl lsp-add sw sw-p3 ovn-nbctl lsp-set-addresses sw-p3 "00:00:00:00:00:03 192.168.0.3" &lt;/pre&gt; &lt;p&gt;And let&amp;#8217;s have a look at our switch ports at this point:&lt;/p&gt; &lt;pre&gt;name : "sw-p3" dynamic_addresses : [] addresses : ["00:00:00:00:00:03 192.168.0.3"] name : "sw-p2" dynamic_addresses : "0a:00:00:00:00:03 192.168.0.2" addresses : [dynamic] name : "sw-p1" dynamic_addresses : "0a:00:00:00:00:04 192.168.0.3" addresses : [dynamic] &lt;/pre&gt; &lt;p&gt;Oops—our new switch port has an address that conflicts with one of our dynamic addresses. This will result in errors when packets are sent. There are a couple of ways to clear this up.&lt;/p&gt; &lt;p&gt;One way to fix this is by clearing the &lt;code&gt;dynamic_addresses&lt;/code&gt; of &lt;code&gt;sw-p2&lt;/code&gt;, and then &lt;code&gt;sw-p2&lt;/code&gt; will get a new dynamic address assigned to it. As mentioned in the previous section, this also means that &lt;code&gt;sw-p2&lt;/code&gt; will get assigned a new MAC address.&lt;/p&gt; &lt;p&gt;The other way is to use &lt;code&gt;ovn-nbctl lsp-set-addresses&lt;/code&gt; on &lt;code&gt;sw-p3&lt;/code&gt; so that it has an address that doesn&amp;#8217;t conflict.&lt;/p&gt; &lt;p&gt;Starting with OVS version 2.10.0, this conflict can no longer occur. Instead, &lt;code&gt;sw-p2&lt;/code&gt; will automatically have its IP address updated to the next available address in the subnet. The code makes the assumption that statically assigned addresses are always correct and that dynamic addresses are &amp;#8220;wrong&amp;#8221; and need to be updated in the case of a conflict.&lt;/p&gt; &lt;p&gt;Starting with OVS version 2.11.0, it will be more difficult to cause this type of conflict. Watch what happens when we try the following with the current master of OVS:&lt;/p&gt; &lt;pre&gt;$ ovn-nbctl lsp-set-addresses sw-p3 "00:00:00:00:00:03 192.168.0.3" ovn-nbctl: Error on switch sw: duplicate IPv4 address 192.168.0.3 &lt;/pre&gt; &lt;p&gt;The message above indicates that the conflict is detected by &lt;code&gt;ovn-nbctl&lt;/code&gt; and the conflicting address is not set on &lt;code&gt;sw-p3&lt;/code&gt;. It still is possible to set a conflicting address on &lt;code&gt;sw-p3&lt;/code&gt; by using the following command:&lt;/p&gt; &lt;pre&gt;# Don't do this! $ ovn-nbctl set Logical_Switch_Port sw-p3 "00:00:00:00:00:03 192.168.0.3" &lt;/pre&gt; &lt;p&gt;Doing this will still result in the conflicting address being set in the northbound database, and it will result in &lt;code&gt;sw-p2&lt;/code&gt; being assigned a new IP address.&lt;/p&gt; &lt;h2&gt;Other fixed problems&lt;/h2&gt; &lt;p&gt;In this final section, we&amp;#8217;ll examine some more minor things that are fixed in the 2.10 series of OVS. These are much less likely to happen than the issues explored in the previous two sections, and they&amp;#8217;re similar. Here&amp;#8217;s a brief summary:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prior to 2.10, if the MAC address on a switch port changes from being statically assigned to dynamically assigned, the MAC address would not be updated. In 2.10+, the MAC address is dynamically assigned.&lt;/li&gt; &lt;li&gt;Prior to 2.10, if the IPv6 address is dynamically assigned and the MAC address on the port changes, then the IPv6 address is not updated. In 2.10+, when the MAC address is changed, the IPv6 address is recalculated too.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;The future of IPAM in OVN&lt;/h2&gt; &lt;p&gt;IPAM offers a handy way to have IP addresses and MAC addresses automatically get assigned to your logical switch ports. In &lt;a href="https://developers.redhat.com/blog/2018/09/03/ovn-dynamic-ip-address-management/"&gt;part 1&lt;/a&gt;, we explored the basics of enabling IPAM in OVN, and in this part, we saw some downsides that have been fixed recently. But what is still to come? New developments are focused not so much on fixing issues as on adding features.&lt;/p&gt; &lt;p&gt;One improvement in the pipe is to allow for the pool of assignable MAC addresses to be configured. As we have seen in these posts, OVN will assign MAC addresses that start with &amp;#8220;0a.&amp;#8221; But what about deployments where you want OVN to assign MAC addresses but you want to pick the range of MAC addresses to be assigned? This is currently being developed. One idea is to provide a start and end address, allowing OVN to assign addresses from that range. Another idea is to allow for an Organizational Unique Identifier (OUI) to be configured and assign OVN addresses using this OUI as a prefix.&lt;/p&gt; &lt;p&gt;Another improvement is to provide consistent pairings of IPv4 addresses and MAC addresses. Currently, OVN assigns MAC and IPv4 addresses independently of each other. However, it would be more friendly on ARP tables to try to assign the same IPv4 address with the same MAC address each time.&lt;/p&gt; &lt;p&gt;Both of the above ideas are currently in development, with a target of being available in the 2.11 series of OVS. I&amp;#8217;m sure those of you reading these blog posts have ideas for further features that could be added. If you do, feel free to leave a comment on this post with your suggestion.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F27%2Fdynamic-ip-address-management-in-open-virtual-network-ovn-part-two%2F&amp;#38;linkname=Dynamic%20IP%20address%20management%20in%20Open%20Virtual%20Network%20%28OVN%29%3A%20Part%20Two" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F27%2Fdynamic-ip-address-management-in-open-virtual-network-ovn-part-two%2F&amp;#38;linkname=Dynamic%20IP%20address%20management%20in%20Open%20Virtual%20Network%20%28OVN%29%3A%20Part%20Two" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F27%2Fdynamic-ip-address-management-in-open-virtual-network-ovn-part-two%2F&amp;#38;linkname=Dynamic%20IP%20address%20management%20in%20Open%20Virtual%20Network%20%28OVN%29%3A%20Part%20Two" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F27%2Fdynamic-ip-address-management-in-open-virtual-network-ovn-part-two%2F&amp;#38;linkname=Dynamic%20IP%20address%20management%20in%20Open%20Virtual%20Network%20%28OVN%29%3A%20Part%20Two" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F27%2Fdynamic-ip-address-management-in-open-virtual-network-ovn-part-two%2F&amp;#38;linkname=Dynamic%20IP%20address%20management%20in%20Open%20Virtual%20Network%20%28OVN%29%3A%20Part%20Two" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F27%2Fdynamic-ip-address-management-in-open-virtual-network-ovn-part-two%2F&amp;#38;linkname=Dynamic%20IP%20address%20management%20in%20Open%20Virtual%20Network%20%28OVN%29%3A%20Part%20Two" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F27%2Fdynamic-ip-address-management-in-open-virtual-network-ovn-part-two%2F&amp;#38;linkname=Dynamic%20IP%20address%20management%20in%20Open%20Virtual%20Network%20%28OVN%29%3A%20Part%20Two" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F27%2Fdynamic-ip-address-management-in-open-virtual-network-ovn-part-two%2F&amp;#38;linkname=Dynamic%20IP%20address%20management%20in%20Open%20Virtual%20Network%20%28OVN%29%3A%20Part%20Two" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F27%2Fdynamic-ip-address-management-in-open-virtual-network-ovn-part-two%2F&amp;#38;title=Dynamic%20IP%20address%20management%20in%20Open%20Virtual%20Network%20%28OVN%29%3A%20Part%20Two" data-a2a-url="https://developers.redhat.com/blog/2018/09/27/dynamic-ip-address-management-in-open-virtual-network-ovn-part-two/" data-a2a-title="Dynamic IP address management in Open Virtual Network (OVN): Part Two"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/09/27/dynamic-ip-address-management-in-open-virtual-network-ovn-part-two/"&gt;Dynamic IP address management in Open Virtual Network (OVN): Part Two&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/acOTlmbiDGU" height="1" width="1" alt=""/&gt;</content><summary>In part one of this series, we explored the dynamic IP address management (IPAM) capabilities of Open Virtual Network. We covered the subnet, ipv6_prefix, and exclude_ips options on logical switches. We then saw how these options get applied to logical switch ports whose addresses have been set to the special “dynamic” value.  OVN, a subproject of Open vSwitch, is used for virtual networking in a ...</summary><dc:creator>Mark Michelson</dc:creator><dc:date>2018-09-27T19:37:44Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/09/27/dynamic-ip-address-management-in-open-virtual-network-ovn-part-two/</feedburner:origLink></entry><entry><title>Keycloak 4.5.0.Final Released</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/WE3eMcyLgN4/keycloak-450final-released.html" /><category term="feed_group_name_keycloak" scheme="searchisko:content:tags" /><category term="feed_name_keycloak" scheme="searchisko:content:tags" /><author><name>Stian Thorgersen</name></author><id>searchisko:content:id:jbossorg_blog-keycloak_4_5_0_final_released</id><updated>2018-09-27T06:58:43Z</updated><published>2018-09-27T06:58:00Z</published><content type="html">&lt;p&gt;To download the release go to the &lt;a href="http://www.keycloak.org/downloads"&gt;Keycloak homepage&lt;/a&gt;. &lt;p&gt;For details on what is included in the release check out the &lt;a href="https://www.keycloak.org/docs/latest/release_notes/index.html"&gt;Release notes&lt;/a&gt; &lt;p&gt;The full list of resolved issues is available in &lt;a href="https://issues.jboss.org/issues/?jql=project%20%3D%20keycloak%20and%20fixVersion%20%3D%204.5.0.Final"&gt;JIRA&lt;/a&gt;. &lt;p&gt;Before you upgrade remember to backup your database and check the &lt;a href="http://www.keycloak.org/docs/latest/upgrading/index.html"&gt;upgrade guide&lt;/a&gt; for anything that may have changed.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/WE3eMcyLgN4" height="1" width="1" alt=""/&gt;</content><summary>To download the release go to the Keycloak homepage. For details on what is included in the release check out the Release notes The full list of resolved issues is available in JIRA. Before you upgrade remember to backup your database and check the upgrade guide for anything that may have changed.</summary><dc:creator>Stian Thorgersen</dc:creator><dc:date>2018-09-27T06:58:00Z</dc:date><feedburner:origLink>http://blog.keycloak.org/2018/09/keycloak-450final-released.html</feedburner:origLink></entry><entry><title>Source versus binary S2I workflows with Red Hat OpenShift Application Runtimes</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/VNVL4r0yZBg/" /><category term="binaries" scheme="searchisko:content:tags" /><category term="binary workflow" scheme="searchisko:content:tags" /><category term="Container Development Kit" scheme="searchisko:content:tags" /><category term="Containers" scheme="searchisko:content:tags" /><category term="Fabric8 Maven Plug-in" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="FMP" scheme="searchisko:content:tags" /><category term="Modern App Dev" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Application Runtimes" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Container Platform" scheme="searchisko:content:tags" /><category term="S21" scheme="searchisko:content:tags" /><category term="source workflow" scheme="searchisko:content:tags" /><category term="source-to-image" scheme="searchisko:content:tags" /><author><name>Fernando Lozano</name></author><id>searchisko:content:id:jbossorg_blog-source_versus_binary_s2i_workflows_with_red_hat_openshift_application_runtimes</id><updated>2018-09-26T20:19:11Z</updated><published>2018-09-26T20:19:11Z</published><content type="html">&lt;p&gt;Red Hat OpenShift supports two workflows for building container images for applications: the &lt;em&gt;source&lt;/em&gt; and the &lt;em&gt;binary&lt;/em&gt; workflows. The binary workflow is the primary focus of the &lt;a href="https://developers.redhat.com/products/rhoar/overview/"&gt;Red Hat OpenShift Application Runtimes&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/fuse/overview/"&gt;Red Hat Fuse&lt;/a&gt; product documentation and training, while the source workflow is the focus of most of the &lt;a href="https://developers.redhat.com/products/openshift/overview/"&gt;Red Hat OpenShift Container Platform&lt;/a&gt; product documentation and training. All of the standard OpenShift Quick Application Templates are based on the source workflow.&lt;/p&gt; &lt;p&gt;A developer might ask, “Can I use both workflows on the same project?” or, “Is there a reason to prefer one workflow over the other?” As a member of the team that developed Red Hat certification training for OpenShift and Red Hat Fuse, I had these questions myself and I hope that this article helps you find your own answers to these questions.&lt;/p&gt; &lt;p&gt;&lt;span id="more-519917"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Comparing the binary and source workflows&lt;/h2&gt; &lt;p&gt;Because both workflows are based on the &lt;i&gt;source-to-image (S2I)&lt;/i&gt; feature, it may sound strange having a &lt;em&gt;binary&lt;/em&gt; option. Actually, both workflows rely on S2I builds using the &lt;em&gt;Source&lt;/em&gt; strategy. The key difference is that the source workflow generates deployable artifacts of your application inside OpenShift, while the binary workflow generates these binary artifacts outside OpenShift. Both of them build the application container image inside OpenShift.&lt;/p&gt; &lt;p&gt;In a sense, the binary workflow provides an application binary as the source for an OpenShift S2I build that generates a container image.&lt;/p&gt; &lt;p&gt;Take a simple application, such as the Vert.x-based &amp;#8220;Hello, World&amp;#8221; available on GitHub at &lt;a href="https://github.com/flozanorht/vertx-hello.git"&gt;https://github.com/flozanorht/vertx-hello.git&lt;/a&gt;. It can be run locally, as a standalone Java application, and it can be deployed on OpenShift using both workflows from the same sources.&lt;/p&gt; &lt;h3&gt;Using the binary workflow&lt;/h3&gt; &lt;p&gt;When using the binary workflow, developers would:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Clone the project to a local folder, so they can change the code to their liking and maybe test it outside of OpenShift.&lt;/li&gt; &lt;li&gt;Log in to OpenShift and create a new project.&lt;/li&gt; &lt;li&gt;Use the &lt;code&gt;mvn&lt;/code&gt; command, which in turn uses the &lt;em&gt;Fabric8 Maven Plug-in (FMP)&lt;/em&gt; to build the container image and create the OpenShift resources that describe the application. Maven performs the following tasks: &lt;ol type="a"&gt; &lt;li&gt;Generates the application package (an executable JAR)&lt;/li&gt; &lt;li&gt;Starts a binary build that creates the application container image, and streams the application package to the build pod&lt;/li&gt; &lt;li&gt;Creates OpenShift resources to deploy the application&lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt;Use either curl or a web browser to test the application.&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Using the source workflow&lt;/h3&gt; &lt;p&gt;When using the source workflow, developers would:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Clone the project to a local folder, so they can change the code to their liking and maybe test it outside of OpenShift.&lt;/li&gt; &lt;li&gt;Commit and push any changes to the origin git repository.&lt;/li&gt; &lt;li&gt;Log in to OpenShift and create a new project.&lt;/li&gt; &lt;li&gt;Use the &lt;code&gt;oc new-app&lt;/code&gt; command to build the container image and create the OpenShift resources that describe the application. &lt;ol type="a"&gt; &lt;li&gt;The OpenShift client command creates OpenShift resources to build and deploy the application.&lt;/li&gt; &lt;li&gt;The build configuration resource starts a source build that runs Maven to generate the application package (JAR) and create the application container image containing the application package.&lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt;Expose the application service to the outside world.&lt;/li&gt; &lt;li&gt;Use either curl or a web browser to test the application.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;From an operational perspective, the main difference between both workflows lies between using the &lt;code&gt;mvn&lt;/code&gt; command or the &lt;code&gt;oc new-app&lt;/code&gt; command:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The binary workflow relies on Maven to perform the heavy lifting, which most Java developers appreciate.&lt;/li&gt; &lt;li&gt;The source workflow, on the other hand, relies on the OpenShift client (the &lt;code&gt;oc&lt;/code&gt; command).&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;When to use each workflow&lt;/h3&gt; &lt;p&gt;Before declaring that you prefer the binary workflow because you already know Maven, consider that developers use the OpenShift client to monitor and troubleshoot their applications, so maybe performing a few tasks using Maven is not that big a win.&lt;/p&gt; &lt;p&gt;The source workflow requires that all changes are pushed to a network-accessible git repository, while the binary workflow works with your local changes. This difference comes from the fact that the source workflow builds your Java package (JAR) inside OpenShift, while the binary workflow takes the Java package you built locally without touching your source code.&lt;/p&gt; &lt;p&gt;For some developers, using the binary workflow saves time, because they perform local builds anyway to run unit tests and perform other tasks. For other developers, the source workflow brings the advantage that all heavy work is done by OpenShift. This means that the developer’s workstation does not need to have Maven, a Java compiler, and everything else installed. Instead, developers could use a low-powered PC, or even a tablet, to write their code, commit, and initiate an OpenShift build to test their applications.&lt;/p&gt; &lt;p&gt;Nothing prevents you from using both workflows to meet different goals. For example, developers can use the binary workflow to test their changes in a local minishift instance, including running unit tests, while a QA environment uses the source workflow to build the application container image, triggered by a webhook, and performs a set of integration tests in a dedicated, multi-node OpenShift cluster.&lt;/p&gt; &lt;h2&gt;Deploying an application using the binary and source workflows&lt;/h2&gt; &lt;p&gt;Let’s experiment using both workflows with the &lt;a href="https://github.com/flozanorht/vertx-hello.git"&gt;Vert.x-based &amp;#8220;Hello, World&amp;#8221; application&lt;/a&gt;. Don&amp;#8217;t worry if you are unfamiliar with Vert.x, because the application is very simple and ready to run. Focus on the Maven POM settings that allow the same project to work with both the binary and the source workflows.&lt;/p&gt; &lt;p&gt;Unlike most examples you&amp;#8217;ve probably seen elsewhere, my Vert.x application is configured to use supported dependencies from Red Hat OpenShift Application Runtimes instead of the upstream community artifacts. Do you need to be a Red Hat paying customer to follow the instructions? Not at all: you just need to register at the &lt;a href="http://developers.redhat.com/"&gt;Red Hat Developers website&lt;/a&gt;, and you&amp;#8217;ll get a free developer&amp;#8217;s subscription to Red Hat Enterprise Linux, Red Hat OpenShift Container Platform, Red Hat OpenShift Application Runtimes, and all of Red Hat&amp;#8217;s middleware and DevOps portfolio.&lt;/p&gt; &lt;p&gt;Fire up your minishift instance, and let&amp;#8217;s try both the source and binary builds on OpenShift. If you do not have a minishift instance, register with Red Hat Developers, &lt;a href="https://developers.redhat.com/products/cdk/download/"&gt;download and install the Red Hat Container Development Kit (CDK)&lt;/a&gt;, and follow the &lt;a href="https://developers.redhat.com/products/cdk/hello-world/"&gt;instructions to set up minishift&lt;/a&gt;, which provides a VM to run OpenShift in your local machine. Or, if you have access to a real OpenShift cluster, log in to it and follow the instructions in the next sections.&lt;/p&gt; &lt;h2&gt;Vert.x &amp;#8220;Hello, World&amp;#8221; using the binary workflow&lt;/h2&gt; &lt;p&gt;First, clone the sample Vert.x application. The following commands assume you are using a Linux machine, but it should not be hard to adapt them to a Windows or Mac machine. You can run the CDK in either of them.&lt;/p&gt; &lt;pre&gt;$ git clone https://github.com/flozanorht/vertx-hello.git $ cd vertx-hello&lt;/pre&gt; &lt;p&gt;Because we are using supported Maven artifacts, you need to configure your Maven installation to use the Red Hat Maven Repository. The &lt;code&gt;conf&lt;/code&gt; folder contains a sample Maven &lt;code&gt;settings.xml&lt;/code&gt; file that you can copy to your &lt;code&gt;~/.m2&lt;/code&gt; folder or use as an example of the changes to make.&lt;/p&gt; &lt;p&gt;Log in to OpenShift and create a test project. The following instructions assume you are using minishift and that your minishift instance is already running, but it should not be hard to adapt them to an external OpenShift cluster.&lt;/p&gt; &lt;pre&gt;$ oc login -u developer -p developer $ oc new-project binary&lt;/pre&gt; &lt;p&gt;Now comes the fun part: let the Fabric8 Maven Plug-in (FMP) do all the work.&lt;/p&gt; &lt;pre&gt;$ mvn -Popenshift fabric8:deploy … [INFO] Building Vert.x Hello, World 1.0 … [INFO] --- fabric8-maven-plugin:3.5.38:resource (fmp) @ vertx-hello --- [INFO] F8: Running in OpenShift mode [INFO] F8: Using docker image name of namespace: binary [INFO] F8: Running generator vertx [INFO] F8: vertx: Using ImageStreamTag 'redhat-openjdk18-openshift:1.3' as builder image … [INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ vertx-hello --- … Tests run: 3, Failures: 0, Errors: 0, Skipped: 0 … [INFO] --- fabric8-maven-plugin:3.5.38:build (fmp) @ vertx-hello --- [INFO] F8: Using OpenShift build with strategy S2I … [INFO] F8: Starting S2I Java Build ..... [INFO] F8: S2I binary build from fabric8-maven-plugin detected [INFO] F8: Copying binaries from /tmp/src/maven to /deployments ... [INFO] F8: ... done [INFO] F8: Pushing image 172.30.1.1:5000/binary/vertx-hello:1.0 … … [INFO] F8: Pushed 6/6 layers, 100% complete [INFO] F8: Push successful [INFO] F8: Build vertx-hello-s2i-1 Complete … [INFO] --- fabric8-maven-plugin:3.5.38:deploy (default-cli) @ vertx-hello --- … [INFO] BUILD SUCCESS …&lt;/pre&gt; &lt;p&gt;The configurations for the FMP are inside the Maven profile named &lt;code&gt;openshift&lt;/code&gt;. This profile allows you to build and run the application locally, without OpenShift. If you want to, invoke the Maven &lt;code&gt;package&lt;/code&gt; goal and run the JAR package from the target folder using &lt;code&gt;java -jar&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The build takes some time; most of it is for downloading Maven artifacts. Generating and pushing the container image to the internal registry takes from a few seconds to a few minutes.&lt;/p&gt; &lt;p&gt;The FMP may lose the connection to the builder pod and display warning messages such as the following, which you can just ignore:&lt;/p&gt; &lt;pre&gt;[INFO] Current reconnect backoff is 1000 milliseconds (T0)&lt;/pre&gt; &lt;p&gt;The following error message can also be ignored:&lt;/p&gt; &lt;pre&gt;[ERROR] Exception in reconnect java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@741d5132 rejected from …&lt;/pre&gt; &lt;p&gt;The FMP already fixed these issues and the fixes should land in Red Hat OpenShift Application Runtimes in the near future.&lt;/p&gt; &lt;p&gt;Despite these messages, your build is successful. The FMP also created a route to allow access to your application. Find the host name assigned to your route:&lt;/p&gt; &lt;pre&gt;$ oc get route NAME          HOST/PORT                                  PATH     SERVICES      PORT      TERMINATION   WILDCARD vertx-hello   vertx-hello-binary.192.168.42.180.nip.io            vertx-hello   8080          None&lt;/pre&gt; &lt;p&gt;And then test your application using curl:&lt;/p&gt; &lt;pre&gt;$ curl http://vertx-hello-binary.192.168.42.180.nip.io/api/hello/Binary Hello Binary, from vertx-hello-binary.192.168.42.180.nip.io.&lt;/pre&gt; &lt;h3&gt;OpenShift resources with the binary workflow&lt;/h3&gt; &lt;p&gt;Note that the FMP also creates a few OpenShift resources: a service, a deployment configuration, and application pods:&lt;/p&gt; &lt;pre&gt;$ oc status In project binary on server https://192.168.42.180:8443 http://vertx-hello-binary.192.168.42.180.nip.io to pod port 8080 (svc/vertx-hello)  dc/vertx-hello deploys istag/vertx-hello:1.0 &amp;#60;-     bc/vertx-hello-s2i source builds uploaded code on openshift/redhat-openjdk18-openshift:1.3     deployment #1 deployed 8 minutes ago - 1 pod …&lt;/pre&gt; &lt;p&gt;The FMP creates OpenShift resources internal defaults. You can provide resource fragment files stored in the &lt;code&gt;src/main/fabric8 project&lt;/code&gt; folder to override these defaults. If you want to customize a readiness probe or resource limits for your pods, you have to edit these files.&lt;/p&gt; &lt;p&gt;You can peek inside the OpenShift build configuration that the FMP created for you. Note that the strategy is &lt;code&gt;Source&lt;/code&gt; and there is a binary input.&lt;/p&gt; &lt;pre&gt;$ oc get bc NAME              TYPE      FROM      LATEST vertx-hello-s2i   Source    Binary    1 $ oc describe bc vertx-hello-s2i … Strategy:    Source From Image:    ImageStreamTag openshift/redhat-openjdk18-openshift:1.3 Output to:    ImageStreamTag vertx-hello:1.0 Binary:       provided on build …&lt;/pre&gt; &lt;h3&gt;Rebuilds with the binary workflow&lt;/h3&gt; &lt;p&gt;The fact that the build requires a binary input means that you cannot simply start a new build using the  &lt;code&gt;oc start-build&lt;/code&gt; command. You also cannot use OpenShift webhooks to start a new build. If you need to perform a new build of the application, use the &lt;code&gt;mvn&lt;/code&gt; command again:&lt;/p&gt; &lt;pre&gt;$ mvn -Popenshift fabric8:deploy … [INFO] Building Vert.x Hello, World 1.0 … Tests run: 3, Failures: 0, Errors: 0, Skipped: 0 … [INFO] --- fabric8-maven-plugin:3.5.38:build (fmp) @ vertx-hello --- [INFO] F8: Using OpenShift build with strategy S2I … [INFO] F8: Pushed 6/6 layers, 100% complete [INFO] F8: Push successful [INFO] F8: Build vertx-hello-s2i-2 Complete … [INFO] BUILD SUCCESS …&lt;/pre&gt; &lt;p&gt;In the end, you get a new container image and a new application pod. Though the build logs seem to imply that the FMP re-creates (or updates) the OpenShift resources, it actually leaves them unchanged. If you need the FMP to update the OpenShift resources, you need to invoke the &lt;code&gt;fabric8:undeploy&lt;/code&gt; Maven goal and then invoke &lt;code&gt;fabric8:deploy&lt;/code&gt; again.&lt;/p&gt; &lt;h1&gt;Vert.x &amp;#8220;Hello, World&amp;#8221; using the source workflow&lt;/h1&gt; &lt;p&gt;First of all, clone the sample Vert.x application. The following commands assume you are using a Linux machine, but it should not be hard to adapt them to a Windows or Mac machine. You can run the CDK in either of them. If you have already cloned from the previous instructions, you can reuse the same cloned git repository and skip the next commands.&lt;/p&gt; &lt;pre&gt;$ git clone https://github.com/flozanorht/vertx-hello.git $ cd vertx-hello&lt;/pre&gt; &lt;p&gt;There is no need to configure Maven to use the Red Hat Maven Repository. The OpenShift builder images are already preconfigured with them. You will not perform local Maven builds anymore.&lt;/p&gt; &lt;p&gt;Log in to OpenShift and create a test project. The following instructions assume you are using minishift, but it should not be hard to adapt them to an external OpenShift cluster.&lt;/p&gt; &lt;pre&gt;$ oc login -u developer -p developer $ oc new-project source&lt;/pre&gt; &lt;p&gt;Now comes the fun part: let OpenShift&amp;#8217;s S2I feature do all the work.&lt;/p&gt; &lt;pre&gt;$ oc new-app redhat-openjdk18-openshift:1.3~https://github.com/flozanorht/vertx-hello.git … --&amp;#62; Creating resources ...     imagestream "vertx-hello" created     buildconfig "vertx-hello" created     deploymentconfig "vertx-hello" created     service "vertx-hello" created --&amp;#62; Success     Build scheduled, use 'oc logs -f bc/vertx-hello' to track its progress. …&lt;/pre&gt; &lt;p&gt;As suggested by the &lt;code&gt;oc new-app&lt;/code&gt; command, follow the OpenShift build logs, which includes the Maven build logs:&lt;/p&gt; &lt;pre&gt;$ oc logs -f bc/vertx-hello -f Cloning "https://github.com/flozanorht/vertx-hello.git" ... … Starting S2I Java Build ..... Maven build detected Initialising default settings /tmp/artifacts/configuration/settings.xml Setting MAVEN_OPTS to -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:+UseParallelOldGC -XX:MinHeapFreeRatio=10 -XX:MaxHeapFreeRatio=20 -XX:GCTimeRatio=4 -XX:AdaptiveSizePolicyWeight=90 -XX:MaxMetaspaceSize=100m -XX:+ExitOnOutOfMemoryError Found pom.xml ... … [INFO] Building Vert.x Hello, World 1.0 … [INFO] BUILD SUCCESS … Copying Maven artifacts from /tmp/src/target to /deployments ... … Pushed 6/6 layers, 100% complete Push successful&lt;/pre&gt; &lt;p&gt;Note that the source build invokes Maven with arguments that prevent it from running the FMP. Also, note that the end result of the build is a container image. The build pushes that container image into the internal registry.&lt;/p&gt; &lt;p&gt;The build takes some time; most of it is for downloading Maven artifacts. Generating and pushing the container image takes from a few seconds to a few minutes.&lt;/p&gt; &lt;p&gt;After the build finishes, you need to create a route to allow access to your application.&lt;/p&gt; &lt;pre&gt;$ oc expose svc vertx-hello route "vertx-hello" exposed&lt;/pre&gt; &lt;p&gt;Now find the host name assigned to your route:&lt;/p&gt; &lt;pre&gt;$ oc get route NAME          HOST/PORT                                  PATH      SERVICES      PORT       TERMINATION   WILDCARD vertx-hello   vertx-hello-source.192.168.42.180.nip.io             vertx-hello   8080-tcp           None&lt;/pre&gt; &lt;p&gt;And then test your application using &lt;code&gt;curl&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;$ curl http://vertx-hello-source.192.168.42.180.nip.io/api/hello/Source Hello Source, from vertx-hello-source.192.168.42.180.nip.io.&lt;/pre&gt; &lt;h3&gt;OpenShift resources with the source workflow&lt;/h3&gt; &lt;p&gt;Note that the &lt;code&gt;oc new-app&lt;/code&gt; command also creates a few OpenShift resources: a service, a deployment configuration, and application pods:&lt;/p&gt; &lt;pre&gt;$ oc status In project source on server https://192.168.42.180:8443 http://vertx-hello-source.192.168.42.180.nip.io to pod port 8080-tcp (svc/vertx-hello)  dc/vertx-hello deploys istag/vertx-hello:latest &amp;#60;-     bc/vertx-hello source builds https://github.com/flozanorht/vertx-hello.git on openshift/redhat-openjdk18-openshift:1.3     deployment #1 deployed 3 minutes ago - 1 pod …&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;oc new-app&lt;/code&gt; command creates OpenShift resources using hard-coded defaults. If you want to customize them, for example, to specify a readiness probe or resource limits for your pods, you have two options:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Find (or create) a suitable OpenShift template, and use this template as input for the &lt;code&gt;oc new-app&lt;/code&gt; command. The templates in the &lt;code&gt;openshift&lt;/code&gt; namespace are a good starting point.&lt;/li&gt; &lt;li&gt;Use OpenShift client commands, such as &lt;code&gt;oc set&lt;/code&gt; and &lt;code&gt;oc edit&lt;/code&gt; to change the resources in place.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can peek inside the OpenShift build configuration that the &lt;code&gt;oc new-app&lt;/code&gt; command created for you. Note that the strategy is &lt;code&gt;Source&lt;/code&gt; and there is a URL input.&lt;/p&gt; &lt;pre&gt;$ oc get bc NAME          TYPE      FROM      LATEST vertx-hello   Source    Git       1 $ oc describe bc vertx-hello … Strategy:    Source URL:       https://github.com/flozanorht/vertx-hello.git From Image:    ImageStreamTag openshift/redhat-openjdk18-openshift:1.3 Output to:    ImageStreamTag vertx-hello:latest …&lt;/pre&gt; &lt;h3&gt;Rebuilds with the source workflow&lt;/h3&gt; &lt;p&gt;If you need to perform a new build of the application, use the &lt;code&gt;oc start-build&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt;$ oc start-build vertx-hello build "vertx-hello-2" started&lt;/pre&gt; &lt;p&gt;You can use the same &lt;code&gt;oc logs&lt;/code&gt; command to watch the OpenShift build logs. In the end, you have a new container image and a new application pod ready and running. You do not need to re-create the route and other OpenShift resources.&lt;/p&gt; &lt;p&gt;Note that Maven downloads all dependencies again because the build pod only uses container ephemeral storage. There is no reuse of the Maven cache between S2I builds by default. Developers performing local Maven builds rely on their local Maven cache to speed up rebuilds. OpenShift provides an incremental builds feature that allows reusing the Maven cache between S2I builds. Incremental builds are a theme of a future post.&lt;/p&gt; &lt;p&gt;If you plan to use source builds, I recommend that you configure a Maven repository server such as Nexus. The &lt;code&gt;MAVEN_MIRROR_URL&lt;/code&gt; build environment variable points to the Maven repository server. This recommendation applies to any organization developing Java applications, but OpenShift makes this need even more pressing.&lt;/p&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;Understanding the OpenShift binary and source workflows allows a developer to make informed decisions about when to use which one. A Continuous Integration/Continuous Delivery (CI/CD) pipeline, managed by Jenkins or any other tool, could use both of them.&lt;/p&gt; &lt;p&gt;The binary and the source workflows make use of the same OpenShift S2I builder images. Both build container images inside OpenShift and push them to the internal registry. For most practical purposes, the container images generated by either the binary or source workflows are equivalent.&lt;/p&gt; &lt;p&gt;The binary workflow may better fit current developer workflows, especially when there is heavy customization of the project&amp;#8217;s POM. The source workflow allows for cloud-based development, such as &lt;a href="https://developers.redhat.com/products/openshiftio/overview/"&gt;Red Hat OpenShift.io&lt;/a&gt; and relieves the developer from needing a high-end workstation.&lt;/p&gt; &lt;p&gt;Red Hat Training provides two developer-oriented courses with content about OpenShift builds:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Red Hat OpenShift Development I: Containerizing Applications (&lt;a href="https://www.redhat.com/en/services/training/do288-red-hat-openshift-development-i-containerizing-applications"&gt;DO288&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Red Hat OpenShift Development II: Creating Microservices with Red Hat OpenShift Application Runtimes (&lt;a href="https://www.redhat.com/en/services/training/red-hat-openshift-development-ii"&gt;DO292&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can also get the book &lt;a href="https://www.openshift.com/deploying-to-openshift/"&gt;Deploying to OpenShift&lt;/a&gt; by Graham Dumpleton for free.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F26%2Fsource-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes%2F&amp;#38;linkname=Source%20versus%20binary%20S2I%20workflows%20with%20Red%20Hat%20OpenShift%20Application%20Runtimes" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F26%2Fsource-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes%2F&amp;#38;linkname=Source%20versus%20binary%20S2I%20workflows%20with%20Red%20Hat%20OpenShift%20Application%20Runtimes" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F26%2Fsource-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes%2F&amp;#38;linkname=Source%20versus%20binary%20S2I%20workflows%20with%20Red%20Hat%20OpenShift%20Application%20Runtimes" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F26%2Fsource-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes%2F&amp;#38;linkname=Source%20versus%20binary%20S2I%20workflows%20with%20Red%20Hat%20OpenShift%20Application%20Runtimes" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F26%2Fsource-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes%2F&amp;#38;linkname=Source%20versus%20binary%20S2I%20workflows%20with%20Red%20Hat%20OpenShift%20Application%20Runtimes" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F26%2Fsource-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes%2F&amp;#38;linkname=Source%20versus%20binary%20S2I%20workflows%20with%20Red%20Hat%20OpenShift%20Application%20Runtimes" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F26%2Fsource-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes%2F&amp;#38;linkname=Source%20versus%20binary%20S2I%20workflows%20with%20Red%20Hat%20OpenShift%20Application%20Runtimes" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F26%2Fsource-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes%2F&amp;#38;linkname=Source%20versus%20binary%20S2I%20workflows%20with%20Red%20Hat%20OpenShift%20Application%20Runtimes" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F26%2Fsource-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes%2F&amp;#38;title=Source%20versus%20binary%20S2I%20workflows%20with%20Red%20Hat%20OpenShift%20Application%20Runtimes" data-a2a-url="https://developers.redhat.com/blog/2018/09/26/source-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes/" data-a2a-title="Source versus binary S2I workflows with Red Hat OpenShift Application Runtimes"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/09/26/source-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes/"&gt;Source versus binary S2I workflows with Red Hat OpenShift Application Runtimes&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/VNVL4r0yZBg" height="1" width="1" alt=""/&gt;</content><summary>Red Hat OpenShift supports two workflows for building container images for applications: the source and the binary workflows. The binary workflow is the primary focus of the Red Hat OpenShift Application Runtimes and Red Hat Fuse product documentation and training, while the source workflow is the focus of most of the Red Hat OpenShift Container Platform product documentation and training. All of ...</summary><dc:creator>Fernando Lozano</dc:creator><dc:date>2018-09-26T20:19:11Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/09/26/source-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes/</feedburner:origLink></entry><entry><title>WildFly Elytron - Credential Store - Next Steps</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/xyLFJ8-I5S0/wildfly-elytron-credential-store-next.html" /><category term="credentialstore" scheme="searchisko:content:tags" /><category term="Elytron" scheme="searchisko:content:tags" /><category term="EncryptedData" scheme="searchisko:content:tags" /><category term="feed_group_name_jbossas" scheme="searchisko:content:tags" /><category term="feed_name_darrans_wildfly_blog" scheme="searchisko:content:tags" /><category term="Java" scheme="searchisko:content:tags" /><category term="security" scheme="searchisko:content:tags" /><category term="vault" scheme="searchisko:content:tags" /><category term="wildfly" scheme="searchisko:content:tags" /><author><name>Darran Lofthouse</name></author><id>searchisko:content:id:jbossorg_blog-wildfly_elytron_credential_store_next_steps</id><updated>2018-09-26T12:52:24Z</updated><published>2018-09-26T12:52:00Z</published><content type="html">During the development of WildFly 11 where we introduced the WildFly Elytron project to the application server one of the new features we added was the credential store.&lt;br /&gt;&lt;br /&gt;&lt;a href="https://developers.redhat.com/blog/2017/12/14/new-jboss-eap-7-1-credential-store/"&gt;https://developers.redhat.com/blog/2017/12/14/new-jboss-eap-7-1-credential-store/&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;Now that we are planning the next stages of development for WildFly 15 and 16 we are revisiting some of the next steps for the credential store, this blog post explores some of the history of the current decisions made with the credential store and the types of enhancement being requested to develop this further.&lt;br /&gt;&lt;br /&gt;Anyone making use of either the CredentialStore or Vault is encouraged to provider your feedback so we can take this into account as the next stages are planned.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Key Differences To Vault&lt;/h3&gt;Prior to the credential store the PicketBox vault was the solution used for the encryption of credentials and other fields within the application server's configuration, the credential store approach was dedicated to the secure storage of credentials.&lt;br /&gt;&lt;br /&gt;Where the PicketBox vault is used in the application server a single Vault is defined across the whole server and aliases from the vault are referenced via expressions in the server's configuration which allows for the values to be retrieved in clear text.&lt;br /&gt;&lt;br /&gt;The credential store on the other hand allows for multiple credential store instances to be defined, resources that make use of the credentials have been updated to a special attribute type where both the name of the credential store can be specified and the alias of the credential within the credential store.&lt;br /&gt;&lt;br /&gt;The credential store resources additionally support management operations to allow for entries within the credential store to be manipulated either by adding / removing entries or by updating existing entries.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Uses of the Store&lt;/h3&gt;Reviewing where the credential store is used we seem to have two predominant scenarios.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Unlocking Local Resources&lt;/h4&gt;In this case a credential is required to unlock a local resource such as a KeyStore, there is no remote authentication to be performed and a credential is generally only required for decrypting the contents of the store.&amp;nbsp; This is the simplest use of the store and once the resource is unlocked it is not likely to need unlocking again.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Accessing Remote Resources&lt;/h4&gt;The second use we see is for services accessing a remote resource, in this case the credentials for the connection are obtained from the credential store.&lt;br /&gt;&lt;br /&gt;This scenario has a reasonable amount of history also attached to the current implementation, it tended to be the case that if you were to access a remote resource it would either not be secured or it would be secured and authentication would require a username and password.&amp;nbsp; Additionally any SSL related configuration would be handled completely independently.&lt;br /&gt;&lt;br /&gt;In recent years however there has been a greater demand for alternative authentication mechanisms, there has been a lot of demand for Kerberos both with the server being given it's own account for authentication and also for the propagation of a remote user authenticated against the server using Kerberos.&amp;nbsp; We haven't seen requests yet for the application server but I suspect OAuth scenarios will be requested soon.&lt;br /&gt;&lt;br /&gt;In both of these cases the security has moved from username / password authentication to more advanced scenarios.&lt;br /&gt;&lt;br /&gt;In parallel to the credential store WildFly Elytron has also introduced an Authentication Client API, this API can be used for configuring the client side authentication policies for various mechanisms, including scenarios that support propagation of the current identity.&amp;nbsp; The authentication client configuration also allows SSLContext configurations to be associated with a specific destination.&lt;br /&gt;&lt;br /&gt;This now raises the question, should services establishing a remote connection which requires authentication and SSL configuration now reference an authentication client configuration instead of the current assumption that a username and credential reference is sufficient?&lt;br /&gt;&lt;br /&gt;This question becomes quite important as it affects the approach we take to a lot of the enhancements requested of us quite significantly.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Next Features&lt;/h3&gt;The features currently being requested against the credential store generally cover three broad areas: -&lt;br /&gt;&lt;br /&gt;&lt;ol&gt;&lt;li&gt;Automation of updates to the store.&lt;/li&gt;&lt;li&gt;Real time updates.&lt;/li&gt;&lt;li&gt;Support for expressions.&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;h4&gt;Automation of Updates to the Store&lt;/h4&gt;The general motivation for this enhancement is to simplify the steps configuring resources which require credentials, using the PicketBox Vault implementation the vault would need to be manipulated offline using a command line tool and then references from the application server configuration.&lt;br /&gt;&lt;br /&gt;The CredentialStore has already moved on from this partially as management operations have been exposed to allow the contents of the store to be directly manipulated using management requests so the store can be manipulated directly from the management tools.&amp;nbsp; However an administrator is still required to operate on the two resources completely independently.&lt;br /&gt;&lt;br /&gt;We predominantly have two options to simplify this further.&lt;br /&gt;&lt;br /&gt;We could take the decision that further enhancement is now a tooling issue, the admin clients could detect a resource is being added that supports credential store references or the password on an existing resource that supports credential store references is being set and provide guidance / automation to persist the credential in the credential store.&lt;br /&gt;&lt;br /&gt;&lt;ol&gt;&lt;li&gt;Would you like to store this credential in a credential store?&lt;/li&gt;&lt;li&gt;Which credential store would you like to use? Or would you like to create a new one?&lt;/li&gt;&lt;li&gt;What alias would you like the credential to be stored under?&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Alternatively, the credential reference attribute supports specifying a reference to a credential store and an alias in the credential store - this attribute however also supports specifying a clear text password.&amp;nbsp; We could automate the manipulation in the management tier and if a clear text password is specified on a resource referencing a credential store automatically add it to the credential store and remove it from the model - if no alias is specified automatically generate one.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;We could also support a combination of the two approaches as in the management tier although we could support the interception of a new clear password if a store needs to be created that would be more involved than we could automate.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4&gt;Real Time Updates&lt;/h4&gt;&lt;div&gt;Where new credentials are added to a credential store using management operations they are already available for use immediately in the application server process without a restart.&amp;nbsp; However we still have some areas to consider further real time update support: -&lt;/div&gt;&lt;div&gt;&lt;ol&gt;&lt;li&gt;Updates to the store on the filesystem.&lt;/li&gt;&lt;li&gt;Complete replacement of an existing store.&lt;/li&gt;&lt;li&gt;Updates to credentials using management operations.&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;In these three cases the primary interest is credentials which are already in use in the application server, however in the case of #1 and #2 it could relate to the addition of new credentials to be used immediately.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;One point to consider is although our default credential store implementation is file based custom implementations could be in use which are not making use of any local files.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;At the credential store level we likely should consider various modes to detect changes when emitting notifications: -&lt;/div&gt;&lt;div&gt;&lt;ol&gt;&lt;li&gt;File system monitoring.&lt;/li&gt;&lt;li&gt;Notifications from within the implementation of the store.&lt;/li&gt;&lt;li&gt;Administrator triggered reloads of either the full store or individual aliases.&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;div&gt;At the service level where a service is making use of a credential it is likely we would want to decide how to handle updates on a service by service basis.&amp;nbsp; It is unlikely we would want to automatically restart / replace services as for some services which make use of credentials this could cause a cascade of service restarts potentially leading the redeployment of deployments.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;I expect some form of notifications will be required, at the coarsest level notifications could be emitted for all services accessing a specific credential store - this however could trigger a significant overhead as a single store could contain a large number of entries used across a large number of resources.&amp;nbsp; Instead we could emit notifications just to the resources using the affected aliases, this would be more efficient from the perspective of the notifications but the complexity now is where a coarse update has been updates to a store such as the underlying file being replaced or a full store refresh being triggered by an administrator we now need to identify which credentials were really modified.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4&gt;Support for Expressions&lt;/h4&gt;&lt;div&gt;It was a deliberate decision to move away from using expressions, however there are still some demands for expressions that need to be considered.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Overall the design decisions within WildFly Elytron have always considered a desire to move away from clear text passwords being present in the application server process, where expressions are used the only route available is to obtain the clear text representation of a String and pass it around the related services.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;One of the enhancements delivered for WildFly 11 was to support multiple credential stores concurrently within the application server, by moving to the complex attribute we were able to make use of capability references to select which credential store to use with a second attribute selecting the alias in the store.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Another consideration was the desire for automatic updates to be applied via the management tier, by moving from expressions to a complex attribute it opens up the options to intercept these values and persist them in the store.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;A further (and possibly the greatest) consideration was the desired support for automatic updates to credentials currently in use in the server, by moving to the capability references aided by the complex attribute definition services can now obtain a direct reference to the store instead of having a credential automatically resolved.&amp;nbsp; By having a reference to a credential store we can potentially add support for direct notifications of updates applied to that store.&amp;nbsp; Where a credential is updated different services may want to respond in different ways, this is why a reference is needed.&amp;nbsp; Within the management tier we can not silently automate the updates, if we were to do so it would likely involve the removal and replacement of the service which could have a side effect of restarting many other services including the deployments.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The biggest restriction of not supporting expressions is attributes for anything other than a credential can no longer be loaded from the credential store - but the missing piece of information is how that is really used and why?&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;As an example usernames could be loaded using expressions, is this because in some environments the username is being considered as sensitive as the credential?&amp;nbsp; Or is it the case that where a credential is loaded from a store it is easier to load the username from the same store.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;If the answer is co-location of the username and password then a more suitable path to look into may be the externalisation of the authentication client configuration allowing the complete client authentication policy to be handled as a single unit.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;If we are still left with attributes in the configuration that need to be stored securely the next question is do they strictly need to be removed from the configuration and looked up from the store?&amp;nbsp; An alternative option we have to consider is supporting the encryption / decryption of Strings inlined in the management model using a credential from the store.&lt;/div&gt;&lt;br /&gt;&lt;h3&gt;Other Enhancements&lt;/h3&gt;&lt;div&gt;Following on from the main enhancements listed above there is also a set of additional enhancements we could consider.&lt;/div&gt;&lt;h4&gt;Injection / Credential Store Access for Deployments&lt;/h4&gt;&lt;div&gt;Deployments can already access the authentication client configuration, however if access to raw credentials is required it may help to access the store - this could additionally mean a deployment could manipulate the store.&lt;/div&gt;&lt;h4&gt;Permission Checks&lt;/h4&gt;&lt;div&gt;Once accessed within deployments, making use of the SecurityIdentity permission checks we could perform permission checks for different read / write operations on the credential store.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4&gt;Auditing&lt;/h4&gt;&lt;div&gt;The credential store could be updated to emit security events as it is accessed, by emitting security events these can be output to audit logs or sent to other event analysing frameworks.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/xyLFJ8-I5S0" height="1" width="1" alt=""/&gt;</content><summary>During the development of WildFly 11 where we introduced the WildFly Elytron project to the application server one of the new features we added was the credential store. https://developers.redhat.com/blog/2017/12/14/new-jboss-eap-7-1-credential-store/ Now that we are planning the next stages of development for WildFly 15 and 16 we are revisiting some of the next steps for the credential store, thi...</summary><dc:creator>Darran Lofthouse</dc:creator><dc:date>2018-09-26T12:52:00Z</dc:date><feedburner:origLink>http://darranl.blogspot.com/2018/09/wildfly-elytron-credential-store-next.html</feedburner:origLink></entry><entry><title>Colloquium - Making sense of enterprise open source software</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/7T6gELFeBdw/colloquium-making-sense-of-enterprise.html" /><category term="conference" scheme="searchisko:content:tags" /><category term="english" scheme="searchisko:content:tags" /><category term="feed_group_name_jbossas" scheme="searchisko:content:tags" /><category term="feed_name_dimitris" scheme="searchisko:content:tags" /><category term="JBoss" scheme="searchisko:content:tags" /><category term="opensource" scheme="searchisko:content:tags" /><category term="redhat" scheme="searchisko:content:tags" /><category term="switzerland" scheme="searchisko:content:tags" /><author><name>Dimitris Andreadis</name></author><id>searchisko:content:id:jbossorg_blog-colloquium_making_sense_of_enterprise_open_source_software</id><updated>2018-09-26T12:40:25Z</updated><published>2018-09-26T12:40:25Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;The coming Friday, Sep/28th @ 2pm, I have the pleasure to be talking at the &lt;a href="https://www3.unifr.ch/inf/en/"&gt;Department of Informatics of the University of Fribourg&lt;/a&gt; on the subject of: &lt;a href="http://mcs.unibnf.ch/events/informatics-colloquium-39"&gt;"Making sense of enterprise open source software"&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Copying here the Abstract from the event &lt;a href="http://mcs.unibnf.ch/sites/diuf.unifr.ch.drupal.mcs/files/downloads/Affiche%20Andreadis.pdf"&gt;flyer&lt;/a&gt;:&lt;br /&gt;&lt;blockquote class="tr_bq"&gt;Red Hat is a leading enterprise software provider that has built a business model around something that is perceived as "free": open source software. In fact, last year Red Hat managed to sell about $3 billion dollars of "free" software and services to the likes of Fortune 500 companies. How can this be possible? How does an open source business model work in practice? Where does it make sense? Why open source has prevailed in so many different technology domains?&lt;br /&gt;&lt;br /&gt;Come to this talk to discover the nuances of enterprise open source software seen from the point of view of JBoss, a popular open source application server project and a start-up company built around it that was acquired by Red Hat back in 2006 to form Red Hat's Middleware division. Also, learn the secrets of how one becomes a successful open source software developer, should you want to get involved with the open source movement, build a career out of it and have a lot of fun on the way.&lt;/blockquote&gt;The event is hosted by &lt;a href="https://www.linkedin.com/in/p-c-m/"&gt;Prof. Philippe Cudré-Mauroux&lt;/a&gt;, whom I'd like to thank for the invitation. It is also perfectly timed so you can be back in Neuchâtel on time for the start of the &lt;a href="http://www.fete-des-vendanges.ch/"&gt;Fête des Vendages&lt;/a&gt;. :)&lt;br /&gt;&lt;br /&gt;See you &lt;a href="https://goo.gl/maps/WipueANDx562"&gt;there&lt;/a&gt;!&lt;br /&gt;&lt;br /&gt;/&lt;a href="http://dandreadis.blogspot.com/"&gt;Dimitris&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/7T6gELFeBdw" height="1" width="1" alt=""/&gt;</content><summary>The coming Friday, Sep/28th @ 2pm, I have the pleasure to be talking at the Department of Informatics of the University of Fribourg on the subject of: "Making sense of enterprise open source software". Copying here the Abstract from the event flyer: Red Hat is a leading enterprise software provider that has built a business model around something that is perceived as "free": open source software. ...</summary><dc:creator>Dimitris Andreadis</dc:creator><dc:date>2018-09-26T12:40:25Z</dc:date><feedburner:origLink>http://dandreadis.blogspot.com/2018/09/colloquium-making-sense-of-enterprise.html</feedburner:origLink></entry><entry><title>Business Applications by jBPM - League of Legends Stats Demo</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/12V-VHvjFg8/business-applications-by-jbpm-league-of.html" /><category term="feed_group_name_jbossjbpmcommunity" scheme="searchisko:content:tags" /><category term="feed_name_swiderskimaciej" scheme="searchisko:content:tags" /><author><name>Tihomir Surdilovic</name></author><id>searchisko:content:id:jbossorg_blog-business_applications_by_jbpm_league_of_legends_stats_demo</id><updated>2018-09-25T14:32:42Z</updated><published>2018-09-25T14:28:00Z</published><content type="html">A lot of our development focus recently has been around business applications, specifically rapid creation/development of business apps that include the full power of jBPM, are easily deployable to the cloud, and are fun and easy to work with.&lt;br /&gt;We are at a point currently where we can start showcasing our work and what better way than with a demo.&lt;br /&gt;&lt;br /&gt;This demo shows off the power of business app generation with jBPM and shows how easy it is to then extend it to create a unique and fun app for your users/customers.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;b&gt;The youtube video for the demo can be found &lt;a href="https://www.youtube.com/watch?v=W-gtooRgOTw"&gt;here&lt;/a&gt;&amp;nbsp;(or click on the image below).&lt;/b&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://www.youtube.com/watch?v=W-gtooRgOTw&amp;amp;t=166s"&gt;&lt;img border="0" data-original-height="1022" data-original-width="1600" height="203" src="https://1.bp.blogspot.com/-wGRwxAc_4OM/W6pGjtscqvI/AAAAAAAAhVE/uLPfFomOe_QaoEw1tEB9-MAj1tSsIZnrQCLcBGAs/s320/Screen%2BShot%2B2018-09-25%2Bat%2B10.41.54%2BAM.png" width="320" /&gt;&lt;/a&gt;&lt;span id="goog_1583922302"&gt;&lt;/span&gt;&lt;a href="https://www.blogger.com/"&gt;&lt;/a&gt;&lt;span id="goog_1583922303"&gt;&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;Feel free to leave any comments/questions/ideas on the video itself or here if you like. The video shows how we generated the business app with start.jbpm.org, extend it and gives alot of good info on how to get up and running with all this.&lt;br /&gt;&lt;br /&gt;And also here are some important links to get you started:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Generate your business app - &lt;a href="http://start.jbpm.org/"&gt;start.jbpm.org&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Read more about business apps and jBPM in &lt;a href="https://docs.jboss.org/jbpm/release/7.11.0.Final/jbpm-docs/html_single/index.html#_businessappoverview"&gt;docs&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;Demo app &lt;a href="https://github.com/business-applications/sample-riot-league-stats"&gt;source code and install instructions&lt;/a&gt;.&lt;/li&gt;&lt;/ul&gt;And some images of the demo app that you can build easily yourself:&lt;br /&gt;&lt;br /&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-u8aIlGABnA8/W6pEb-vhTPI/AAAAAAAAhU4/EY8pQCghSSY7NViiF62wZQoCsvM9AEPUQCLcBGAs/s1600/Screen%2BShot%2B2018-09-25%2Bat%2B10.33.31%2BAM.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="930" data-original-width="1600" height="186" src="https://1.bp.blogspot.com/-u8aIlGABnA8/W6pEb-vhTPI/AAAAAAAAhU4/EY8pQCghSSY7NViiF62wZQoCsvM9AEPUQCLcBGAs/s320/Screen%2BShot%2B2018-09-25%2Bat%2B10.33.31%2BAM.png" width="320" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Demo app - Summoner Search&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;br /&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://2.bp.blogspot.com/-pIwFY6Xqidw/W6pD_m4CTBI/AAAAAAAAhUw/g02xa2kgowQuObuSmqYRJF8MKKN4PguhQCLcBGAs/s1600/Screen%2BShot%2B2018-09-25%2Bat%2B10.00.27%2BAM.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1100" data-original-width="1600" height="219" src="https://2.bp.blogspot.com/-pIwFY6Xqidw/W6pD_m4CTBI/AAAAAAAAhUw/g02xa2kgowQuObuSmqYRJF8MKKN4PguhQCLcBGAs/s320/Screen%2BShot%2B2018-09-25%2Bat%2B10.00.27%2BAM.png" width="320" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Demo app - Match Results&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;Let us know what you think. A lot more info about this is coming your way so stay tuned and have fun generating your business apps!&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/12V-VHvjFg8" height="1" width="1" alt=""/&gt;</content><summary>A lot of our development focus recently has been around business applications, specifically rapid creation/development of business apps that include the full power of jBPM, are easily deployable to the cloud, and are fun and easy to work with. We are at a point currently where we can start showcasing our work and what better way than with a demo. This demo shows off the power of business app gener...</summary><dc:creator>Tihomir Surdilovic</dc:creator><dc:date>2018-09-25T14:28:00Z</dc:date><feedburner:origLink>http://mswiderski.blogspot.com/2018/09/business-applications-by-jbpm-league-of.html</feedburner:origLink></entry><entry><title>Running Microsoft SQL Server on Red Hat OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/9-6JBAU_qIo/" /><category term=".net" scheme="searchisko:content:tags" /><category term=".NET Core" scheme="searchisko:content:tags" /><category term="CDK" scheme="searchisko:content:tags" /><category term="Container Development Kit" scheme="searchisko:content:tags" /><category term="Containers" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Red Hat Container Development Kit" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Container Platform" scheme="searchisko:content:tags" /><category term="SQL Operation Studio" scheme="searchisko:content:tags" /><category term="SQL Server" scheme="searchisko:content:tags" /><author><name>Tom Deseyn</name></author><id>searchisko:content:id:jbossorg_blog-running_microsoft_sql_server_on_red_hat_openshift</id><updated>2018-09-25T13:25:41Z</updated><published>2018-09-25T13:25:41Z</published><content type="html">&lt;p&gt;In this blog post, we&amp;#8217;ll set up Microsoft SQL Server on &lt;a href="http://openshift.com/"&gt;Red Hat OpenShift&lt;/a&gt;. We&amp;#8217;ll use SQL Server to store data for a simple &lt;a href="https://developers.redhat.com/products/dotnet/overview/"&gt;ASP.NET Core&lt;/a&gt; application running in a container deployed on OpenShift that manages a list of contacts. When we have that set up, we&amp;#8217;ll use SQL Operation Studio to connect to the server running on OpenShift from our developer machine.&lt;/p&gt; &lt;p&gt;&lt;span id="more-520777"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Installing and configuring CDK&lt;/h2&gt; &lt;p&gt;To set this up, we&amp;#8217;ll use &lt;a href="https://developers.redhat.com/products/cdk/overview/"&gt;Red Hat Container Development Kit&lt;/a&gt; (CDK). CDK provides you with all the tools to develop container-based applications. Using CDK, you’ll have OpenShift (and Kubernetes) running in a VM that includes a single-node OpenShift cluster on your Windows, macOS, or Linux machine.&lt;/p&gt; &lt;p&gt;To use CDK, you need a no-cost subscription. When you download CDK from &lt;a href="https://developers.redhat.com"&gt;developers.redhat.com&lt;/a&gt;, a no-cost developer subscription will be automatically added to your account. Follow &lt;a href="https://developers.redhat.com/products/cdk/hello-world/"&gt;these instructions&lt;/a&gt; to install CDK.&lt;/p&gt; &lt;p&gt;Next, we&amp;#8217;ll update the .NET Core versions available on our minishift instance. (If you haven&amp;#8217;t done so already, start minishift.) We&amp;#8217;ll do this in the &lt;code&gt;openshift&lt;/code&gt; project, which will make .NET Core available in all our OpenShift projects. To do this, log in as the administrator and then switch to the &lt;code&gt;openshift&lt;/code&gt; project. If the &lt;code&gt;oc&lt;/code&gt; binary is not on your path, perform the steps described by &lt;code&gt;minishift oc-env&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt;oc login -u system:admin oc project openshift&lt;/pre&gt; &lt;p&gt;Now that we&amp;#8217;ve switched to the &lt;code&gt;openshift&lt;/code&gt; project, update your .NET Core versions by following the steps in &lt;a href="https://access.redhat.com/documentation/en-us/net_core/2.1/html/getting_started_guide/gs_dotnet_on_openshift#install_imagestreams"&gt;Install Image Streams&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;As the last step, we&amp;#8217;ll add SQL Server 2017 support (&lt;code&gt;mssql:2017&lt;/code&gt; image) and two templates to facilitate setting up SQL Server and our .NET Core application.&lt;/p&gt; &lt;pre&gt;oc create -f https://raw.githubusercontent.com/tmds/dotnet-mssql-ex/master/openshift/imagestreams.json oc create -f https://raw.githubusercontent.com/tmds/dotnet-mssql-ex/master/openshift/template.json oc create -f https://raw.githubusercontent.com/tmds/dotnet-mssql-ex/master/app/RazorPagesContacts/dotnet-template.json&lt;/pre&gt; &lt;p&gt;Now we&amp;#8217;ll switch back to the developer user account:&lt;/p&gt; &lt;pre&gt;oc login -u developer&lt;/pre&gt; &lt;h2&gt;Deploying SQL Server&lt;/h2&gt; &lt;p&gt;Open up the OpenShift web UI by running &lt;code&gt;minishift console&lt;/code&gt;. If you haven&amp;#8217;t opened it before, you&amp;#8217;ll need to accept using the self-signed certificate.&lt;/p&gt; &lt;p&gt;Now log in using &lt;code&gt;developer&lt;/code&gt;/&lt;code&gt;developer&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Create a project using the &lt;strong&gt;Create Project&lt;/strong&gt; button at the top right and name the project to &lt;code&gt;mssqldemo&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The UI will switch to the new project and we&amp;#8217;ll add our SQL Server by clicking &lt;strong&gt;Browse Catalog&lt;/strong&gt; and selecting &lt;strong&gt;Microsoft SQL Server&lt;/strong&gt;. The default values will create a SQL server instance named &lt;code&gt;mssql&lt;/code&gt;. You need to accept the EULA by typing &lt;strong&gt;Y&lt;/strong&gt; in the corresponding textbox. Then click &lt;strong&gt;Create&lt;/strong&gt; to deploy SQL Server.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig1-1.png"&gt;&lt;img class=" aligncenter wp-image-520827 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig1-1.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig1-1.png" alt="Adding a SQL server" width="906" height="677" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig1-1.png 906w, https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig1-1-300x224.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig1-1-768x574.png 768w" sizes="(max-width: 906px) 100vw, 906px" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you move to the project overview using the &lt;strong&gt;Overview&lt;/strong&gt; button on the top left, you&amp;#8217;ll see the pod that runs our SQL Server. You can click on the &lt;strong&gt;1 pod&lt;/strong&gt; on the right side, which gives you more info about the pod. Clicking &lt;strong&gt;Logs&lt;/strong&gt; at the top will show you SQL server logging. If you scroll through the log, you&amp;#8217;ll see we&amp;#8217;re running SQL Server on &lt;a href="https://developers.redhat.com/products/rhel/download/"&gt;Red Hat Enterprise Linux&lt;/a&gt; and it is listening for us on port 1433.&lt;/p&gt; &lt;p&gt;As part of our deployment, two interesting resources were created: a secret named &lt;code&gt;mssql-secret&lt;/code&gt;, which stores our SQL Server password and a persistent volume claim (PVC) named &lt;code&gt;mssql-pcc&lt;/code&gt;, which represents the persistent storage from our database server. You can see these resources under &lt;strong&gt;Resources &amp;#62; Secrets&lt;/strong&gt; and &lt;strong&gt;Storage&lt;/strong&gt;, respectively. Take a look at the secret and the password stored inside.&lt;/p&gt; &lt;h2&gt;Using SQL Server from .NET Core&lt;/h2&gt; &lt;p&gt;Go back to the &lt;strong&gt;Overview&lt;/strong&gt;. Now on the top right select &lt;strong&gt;Add to Project &amp;#62; Browse Catalog&lt;/strong&gt;. We&amp;#8217;ll use the &lt;strong&gt;.NET Using Microsoft SQL Server&lt;/strong&gt; template this time. You can use the default parameters. Note that we pass the name of the SQL Server (&lt;code&gt;mssql&lt;/code&gt;) and the name of the secret that contains the password (&lt;code&gt;mssql-secret&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig2.png"&gt;&lt;img class=" aligncenter wp-image-520837 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig2.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig2.png" alt="Adding the SQL server to the project" width="907" height="679" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig2.png 907w, https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig2-300x225.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig2-768x575.png 768w" sizes="(max-width: 907px) 100vw, 907px" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The SQL Server name and secret are passed as environment variables (&lt;code&gt;MSSQL_SERVER&lt;/code&gt; and &lt;code&gt;MSSQL_SA_PASSWORD&lt;/code&gt;) to the .NET Core application as part of the template. Additionally the template sets &lt;code&gt;DB_PROVIDER&lt;/code&gt; to &lt;code&gt;mssql&lt;/code&gt; to configure the application to use SQL Server as the back end. You can see this if you take a look at the &lt;a href="https://raw.githubusercontent.com/tmds/dotnet-mssql-ex/master/app/RazorPagesContacts/dotnet-template.json"&gt;template.json&lt;/a&gt; file.&lt;/p&gt; &lt;p&gt;The code in our application that uses these variables looks like this:&lt;/p&gt; &lt;pre class="aLF-aPX-K0-aPE"&gt;enum DbProvider { Mssql, Memory } public void ConfigureServices(IServiceCollection services) { DbProvider? dbProvider = Configuration.GetValue&amp;#60;DbProvider?&amp;#62;("DB_PROVIDER"); if (dbProvider == null &amp;#38;&amp;#38; !IsOpenShift) { dbProvider = DbProvider.Memory; } switch (dbProvider) { case DbProvider.Mssql: string server = Configuration["MSSQL_SERVER"] ?? "localhost"; string password = Configuration["MSSQL_SA_PASSWORD"]; string user = "sa"; string dbName = "myContacts"; string connectionString = $@"Server={server};Database={dbName};User Id={user};Password={password};"; Logger.LogInformation($"Using SQL Server: {server}"); services.AddDbContext(options =&amp;#62; options.UseSqlServer(connectionString)); break; case DbProvider.Memory: Logger.LogInformation("Using InMemory database"); services.AddDbContext(options =&amp;#62; options.UseInMemoryDatabase("name")); _migrateDatabase = false; break; default: throw new Exception($"Unknown db provider: {dbProvider}"); } services.AddMvc(); } private static bool IsOpenShift =&amp;#62; !string.IsNullOrEmpty(Environment.GetEnvironmentVariable("OPENSHIFT_BUILD_NAME"));&lt;/pre&gt; &lt;pre&gt;&lt;/pre&gt; &lt;p&gt;When our application first starts, it will create the database to store our contacts.&lt;/p&gt; &lt;p&gt;This is done using &lt;a href="https://docs.microsoft.com/en-us/ef/core/managing-schemas/migrations/"&gt;Entity Framework Migrations&lt;/a&gt;. The code to create the database is part of our project and was generated using &lt;code&gt;dotnet ef migrations add Initial&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;To apply the migrations at startup, our project has the following code:&lt;/p&gt; &lt;pre class="aLF-aPX-K0-aPE"&gt;public void Configure(IApplicationBuilder app) { if (_migrateDatabase) { MigrateDatabase(app); } app.UseMvc(); } private static void MigrateDatabase(IApplicationBuilder app) { using (var serviceScope = app.ApplicationServices .GetRequiredService() .CreateScope()) { using (var context = serviceScope.ServiceProvider.GetService()) { context.Database.Migrate(); } } }&lt;/pre&gt; &lt;p&gt;In the &lt;strong&gt;Overview&lt;/strong&gt;, we can see our application being built and deployed. Once the application pod is up and running, click the URL to open up the website. Use the &lt;strong&gt;Create&lt;/strong&gt; link to add a few names to the database.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig3.png"&gt;&lt;img class=" aligncenter wp-image-520847 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig3.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig3.png" alt="Adding a few names to the database" width="380" height="321" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig3.png 380w, https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig3-300x253.png 300w" sizes="(max-width: 380px) 100vw, 380px" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Managing SQL Server using SQL Operations Studio&lt;/h2&gt; &lt;p&gt;SQL Operations Studio is a cross-platform tool for managing SQL Server. You can find the installation instructions in &lt;a href="https://docs.microsoft.com/en-us/sql/sql-operations-studio/download?view=sql-server-2017"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The SQL Server is accessible only within the OpenShift cluster. To be able to connect to it from our machine, we’ll port-forward to the SQL Server pod.&lt;/p&gt; &lt;p&gt;First, we find out the name of the pod running SQL Server:&lt;/p&gt; &lt;pre&gt;$ oc get pod | grep mssql | grep Running dotnet-mssql-example-1-gmdvf 1/1 Running 0 43m mssql-1-l7wn8 1/1 Running 0 1h &lt;/pre&gt; &lt;p&gt;The output from the command shows our SQL Server pod is named &lt;code&gt;mssql-1-l7wn8&lt;/code&gt; and our .NET Core application is named &lt;code&gt;dotnet-mssql-example-1-gmdvf&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Now we can set up the port forwarding:&lt;/p&gt; &lt;pre&gt;oc port-forward mssql-1-l7wn8 1433:1433&lt;/pre&gt; &lt;p&gt;Now open up SQL Operations Studio and add a connection to localhost for user &lt;code&gt;sa&lt;/code&gt; with the password from the &lt;code&gt;mssql-secret&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig4.png"&gt;&lt;img class=" aligncenter wp-image-520857 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig4-1024x803.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig4-1024x803.png" alt="Adding a connection to localhost" width="640" height="502" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig4-1024x803.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig4-300x235.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig4-768x602.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig4.png 1027w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Once we have successfully connected, we can see the databases on the server and their tables. Navigate to the &lt;strong&gt;Customers&lt;/strong&gt; table in the &lt;strong&gt;myContacts&lt;/strong&gt; database.&lt;/p&gt; &lt;p&gt;To see the data in the database, right-click it and click &lt;strong&gt;Select top 1000&lt;/strong&gt;. The query runs, and the data we entered using the ASP.NET Core application shows up in the query results.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig5.png"&gt;&lt;img class=" aligncenter wp-image-520867 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig5.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig5.png" alt="Query results" width="957" height="625" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig5.png 957w, https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig5-300x196.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/09/fig5-768x502.png 768w" sizes="(max-width: 957px) 100vw, 957px" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this blog post, you&amp;#8217;ve learned how to deploy Microsoft SQL Server on OpenShift and use it from an ASP.NET Core application running on OpenShift. You&amp;#8217;ve also seen how you can connect to the database server using SQL Operations Studio.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F25%2Fsql-server-on-openshift%2F&amp;#38;linkname=Running%20Microsoft%20SQL%20Server%20on%20Red%20Hat%20OpenShift" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F25%2Fsql-server-on-openshift%2F&amp;#38;linkname=Running%20Microsoft%20SQL%20Server%20on%20Red%20Hat%20OpenShift" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F25%2Fsql-server-on-openshift%2F&amp;#38;linkname=Running%20Microsoft%20SQL%20Server%20on%20Red%20Hat%20OpenShift" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F25%2Fsql-server-on-openshift%2F&amp;#38;linkname=Running%20Microsoft%20SQL%20Server%20on%20Red%20Hat%20OpenShift" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F25%2Fsql-server-on-openshift%2F&amp;#38;linkname=Running%20Microsoft%20SQL%20Server%20on%20Red%20Hat%20OpenShift" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F25%2Fsql-server-on-openshift%2F&amp;#38;linkname=Running%20Microsoft%20SQL%20Server%20on%20Red%20Hat%20OpenShift" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F25%2Fsql-server-on-openshift%2F&amp;#38;linkname=Running%20Microsoft%20SQL%20Server%20on%20Red%20Hat%20OpenShift" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F25%2Fsql-server-on-openshift%2F&amp;#38;linkname=Running%20Microsoft%20SQL%20Server%20on%20Red%20Hat%20OpenShift" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F25%2Fsql-server-on-openshift%2F&amp;#38;title=Running%20Microsoft%20SQL%20Server%20on%20Red%20Hat%20OpenShift" data-a2a-url="https://developers.redhat.com/blog/2018/09/25/sql-server-on-openshift/" data-a2a-title="Running Microsoft SQL Server on Red Hat OpenShift"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/09/25/sql-server-on-openshift/"&gt;Running Microsoft SQL Server on Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/9-6JBAU_qIo" height="1" width="1" alt=""/&gt;</content><summary>In this blog post, we’ll set up Microsoft SQL Server on Red Hat OpenShift. We’ll use SQL Server to store data for a simple ASP.NET Core application running in a container deployed on OpenShift that manages a list of contacts. When we have that set up, we’ll use SQL Operation Studio to connect to the server running on OpenShift from our developer machine. Installing and configuring CDK To set this ...</summary><dc:creator>Tom Deseyn</dc:creator><dc:date>2018-09-25T13:25:41Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/09/25/sql-server-on-openshift/</feedburner:origLink></entry><entry><title>The future of Java and OpenJDK updates without Oracle support</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/XhMvAWQfyTw/" /><category term="Announcement" scheme="searchisko:content:tags" /><category term="community" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Java" scheme="searchisko:content:tags" /><category term="JDK 11" scheme="searchisko:content:tags" /><category term="JDK 8" scheme="searchisko:content:tags" /><category term="OpenJDK" scheme="searchisko:content:tags" /><category term="OpenJDK 8" scheme="searchisko:content:tags" /><author><name>Andrew Haley</name></author><id>searchisko:content:id:jbossorg_blog-the_future_of_java_and_openjdk_updates_without_oracle_support</id><updated>2018-09-24T16:15:23Z</updated><published>2018-09-24T16:15:23Z</published><content type="html">&lt;p&gt;Oracle recently announced that it would no longer supply free (as in beer) binary downloads for JDK releases after a six-month period, and neither would Oracle engineers write patches for bugs after that period. This has caused a great deal of concern among some Java users.&lt;/p&gt; &lt;p&gt;From my point of view, this is little more than business as usual. Several years ago, the OpenJDK 6 updates (jdk6u) project was relinquished by Oracle and I assumed leadership, and then the same happened with OpenJDK 7. Subsequently, Andrew Brygin of Azul took over the leadership of OpenJDK 6. The OpenJDK Vulnerability Group, with members from many organizations, collaborates on critical security issues. With the help of the wider OpenJDK community and my team at Red Hat, we have continued to provide updates for critical bugs and security vulnerabilities at regular intervals. I can see no reason why this process should not work in the same way for &lt;a href="https://developers.redhat.com/products/openjdk/overview/"&gt;OpenJDK 8&lt;/a&gt; and the next long-term support release, OpenJDK 11.&lt;/p&gt; &lt;p&gt;&lt;span id="more-520407"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;I&amp;#8217;m happy to assume leadership of the JDK 8 and 11 update projects if I have the support of the community.&lt;/p&gt; &lt;p&gt;At Red Hat, we intend to provide support for OpenJDK 8 to our customers until 2023, and our policy of always &amp;#8220;upstream first&amp;#8221; implies that OpenJDK 8 will continue to be updated for critical bugs and security fixes until then. Something similar will happen for JDK 11.&lt;/p&gt; &lt;p&gt;In addition to the people and organizations currently helping with OpenJDK updates, I have received offers of help from organizations not currently involved, in particular from Amazon Web Services. This bodes well, but it may take time to get everyone up to speed working as part of the community.&lt;/p&gt; &lt;p&gt;There is also the question of back-porting important features from later OpenJDK releases to, for example, JDK 8. While new features, particularly performance-related ones, are undoubtedly nice to have, our first priority must be to not break anything: we must remember that we are stewards of a very precious piece of software. Only if we are sure that we&amp;#8217;re not taking unnecessary risks should we do major back-ports. We also have to consider the maintenance burden. So, each proposal will have to be taken on its individual merits, and I don&amp;#8217;t think we can have a one-size-fits-all policy for such things.&lt;/p&gt; &lt;p&gt;One question which frequently arises is that of how people will get free downloads of compiled OpenJDK binaries, as opposed to the source code downloads that are provided by OpenJDK. I believe that the OpenJDK updates project itself should commit to providing binaries when releases are made. (Having said that, if you&amp;#8217;re using some kind of Linux distribution, I would strongly recommend that you use the OpenJDK packages that are provided by the system and its package manager: you should get better integration and ease of updating that way. Some people might be worried that their chosen distribution will not build, test, and package OpenJDK correctly, but if you don&amp;#8217;t trust your distribution to build packages, you shouldn&amp;#8217;t be using it at all.)&lt;/p&gt; &lt;p&gt;So, when we talk about OpenJDK binaries we&amp;#8217;re mainly talking about Windows and Macintosh downloads. It will be up to the JDK updates projects to decide how and where to build the binaries. Having said that, my team at Red Hat is happy to commit to providing regularly updated, tested (and, in particular, TCK&amp;#8217;d) &lt;a href="https://developers.redhat.com/products/openjdk/overview/"&gt;Windows and Linux downloads&lt;/a&gt;, but we probably will need help building and testing on Macintosh. I&amp;#8217;m sure we can get this done and we can continue to deserve the trust of Java users.&lt;/p&gt; &lt;p&gt;Keeping Java updated in the absence of support from Oracle engineers will be a challenge to the Java community, but I believe it is one we should enthusiastically embrace. It is a golden opportunity for us, the community, to show what we can do. A truly open and transparent OpenJDK updates project will encourage wider participation and benefit all Java users.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F24%2Fthe-future-of-java-and-openjdk-updates-without-oracle-support%2F&amp;#38;linkname=The%20future%20of%20Java%20and%20OpenJDK%20updates%20without%20Oracle%20support" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F24%2Fthe-future-of-java-and-openjdk-updates-without-oracle-support%2F&amp;#38;linkname=The%20future%20of%20Java%20and%20OpenJDK%20updates%20without%20Oracle%20support" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F24%2Fthe-future-of-java-and-openjdk-updates-without-oracle-support%2F&amp;#38;linkname=The%20future%20of%20Java%20and%20OpenJDK%20updates%20without%20Oracle%20support" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F24%2Fthe-future-of-java-and-openjdk-updates-without-oracle-support%2F&amp;#38;linkname=The%20future%20of%20Java%20and%20OpenJDK%20updates%20without%20Oracle%20support" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F24%2Fthe-future-of-java-and-openjdk-updates-without-oracle-support%2F&amp;#38;linkname=The%20future%20of%20Java%20and%20OpenJDK%20updates%20without%20Oracle%20support" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F24%2Fthe-future-of-java-and-openjdk-updates-without-oracle-support%2F&amp;#38;linkname=The%20future%20of%20Java%20and%20OpenJDK%20updates%20without%20Oracle%20support" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F24%2Fthe-future-of-java-and-openjdk-updates-without-oracle-support%2F&amp;#38;linkname=The%20future%20of%20Java%20and%20OpenJDK%20updates%20without%20Oracle%20support" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F24%2Fthe-future-of-java-and-openjdk-updates-without-oracle-support%2F&amp;#38;linkname=The%20future%20of%20Java%20and%20OpenJDK%20updates%20without%20Oracle%20support" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F24%2Fthe-future-of-java-and-openjdk-updates-without-oracle-support%2F&amp;#38;title=The%20future%20of%20Java%20and%20OpenJDK%20updates%20without%20Oracle%20support" data-a2a-url="https://developers.redhat.com/blog/2018/09/24/the-future-of-java-and-openjdk-updates-without-oracle-support/" data-a2a-title="The future of Java and OpenJDK updates without Oracle support"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/09/24/the-future-of-java-and-openjdk-updates-without-oracle-support/"&gt;The future of Java and OpenJDK updates without Oracle support&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/XhMvAWQfyTw" height="1" width="1" alt=""/&gt;</content><summary>Oracle recently announced that it would no longer supply free (as in beer) binary downloads for JDK releases after a six-month period, and neither would Oracle engineers write patches for bugs after that period. This has caused a great deal of concern among some Java users. From my point of view, this is little more than business as usual. Several years ago, the OpenJDK 6 updates (jdk6u) project w...</summary><dc:creator>Andrew Haley</dc:creator><dc:date>2018-09-24T16:15:23Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/09/24/the-future-of-java-and-openjdk-updates-without-oracle-support/</feedburner:origLink></entry><entry><title>This week in JBoss (21st September 2018) - a mixed bag</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/RVNTUI0OaBQ/this-week-in-jboss-21st-september-2018-a-mixed-bag" /><category term="Camel" scheme="searchisko:content:tags" /><category term="feed_group_name_global" scheme="searchisko:content:tags" /><category term="feed_name_weeklyeditorial" scheme="searchisko:content:tags" /><category term="hibernate;" scheme="searchisko:content:tags" /><category term="jakarta ee" scheme="searchisko:content:tags" /><category term="JavaZone" scheme="searchisko:content:tags" /><category term="narayana" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="stm" scheme="searchisko:content:tags" /><author><name>Mark Little</name></author><id>searchisko:content:id:jbossorg_blog-this_week_in_jboss_21st_september_2018_a_mixed_bag</id><updated>2018-09-21T11:38:18Z</updated><published>2018-09-21T11:38:00Z</published><content type="html">&lt;!-- [DocumentBodyStart:97507437-318e-4240-84b7-0994a57ac4df] --&gt;&lt;div class="jive-rendered-content"&gt;&lt;p&gt;So in this week's entry we have a mixed bag of things to cover with no single focus area - the teams have been working across a number of areas.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;OK so let's kick it off with a long entry from Michael Musgrove on the &lt;a class="jive-link-external-small" href="http://narayana.io/" rel="nofollow"&gt;Narayana&lt;/a&gt; team about &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/tips_on_how_to_evaluate_stm_implementations" rel="nofollow"&gt;how to evaluate STM implementations&lt;/a&gt;. As Mike says at the start of the article:&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;"Software Transactional Memory (STM) is a way of providing transactional behaviour for threads operating on shared memory. The transaction is an atomic and isolated set of changes to memory such that prior to commit no other thread sees the memory updates and after commit the changes appear to take effect instantaneously so other threads never see partial updates but on abort all of the updates are discarded."&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;This is the first article in a series so if you're interested in transactions (and let's face it, who isn't?!) keep watching the &lt;a class="jive-link-external-small" href="http://jbossts.blogspot.com/" rel="nofollow"&gt;Narayana blog&lt;/a&gt;.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;There are a few entries on Hibernate to report on this week. The first from Guillaume talks about how to use &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/using_hibernate_orm_with_jdk_11" rel="nofollow"&gt;Hibernate ORM with JDK 11&lt;/a&gt;. Then Yoann announced the &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/triple_bugfix_release_for_hibernate_search_5_10_5_9_5_6" rel="nofollow"&gt;release of Hibernate Search 5.6, 5.9 and 5.10&lt;/a&gt;. And of course no editorial would be complete without referencing the regular &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/hibernate_community_newsletter_18_2018" rel="nofollow"&gt;Hibernate Community Newsletter&lt;/a&gt;!&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;Meanwhile Mr Camel himself, Claus Ibsen, has been to JavaZone and &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/my_trip_to_javazone_2018" rel="nofollow"&gt;has written a report&lt;/a&gt;, a photo from which is below!&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;a href="https://developer.jboss.org/servlet/JiveServlet/showImage/38-6225-247599/IMG_7011.jpg"&gt;&lt;img alt="" class="image-1 jive-image" height="240" src="https://developer.jboss.org/servlet/JiveServlet/downloadImage/38-6225-247599/IMG_7011.jpg" style="height: auto;" width="320"/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;Finally for this week Cheng has written an article on &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/build_and_deploy_containerized_java_batch_applications_on_openshift" rel="nofollow"&gt;building and deploying containerized Java Batch Applications on OpenShift!&lt;/a&gt;&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;OK that's it for now! See you next time.&lt;/p&gt;&lt;/div&gt;&lt;!-- [DocumentBodyEnd:97507437-318e-4240-84b7-0994a57ac4df] --&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/RVNTUI0OaBQ" height="1" width="1" alt=""/&gt;</content><summary>So in this week's entry we have a mixed bag of things to cover with no single focus area - the teams have been working across a number of areas.   OK so let's kick it off with a long entry from Michael Musgrove on the Narayana team about how to evaluate STM implementations. As Mike says at the start of the article:   "Software Transactional Memory (STM) is a way of providing transactional behaviou...</summary><dc:creator>Mark Little</dc:creator><dc:date>2018-09-21T11:38:00Z</dc:date><feedburner:origLink>https://developer.jboss.org/blogs/weekly-editorial/2018/09/21/this-week-in-jboss-21st-september-2018-a-mixed-bag</feedburner:origLink></entry></feed>
